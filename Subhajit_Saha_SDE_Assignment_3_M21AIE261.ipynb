{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7efe21",
   "metadata": {},
   "source": [
    "### Subhajit Saha - M21AIE261\n",
    "### SDE - Assignment 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfad78b",
   "metadata": {},
   "source": [
    "### Dataset source : https://www.kaggle.com/datasets/debashis74017/stock-market-data-nifty-50-stocks-1-min-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a99d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import functools\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84861e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADANIPORTS_with_indicators_.csv',\n",
       " 'ASIANPAINT_with_indicators_.csv',\n",
       " 'AXISBANK_with_indicators_.csv',\n",
       " 'BAJAJFINSV_with_indicators_.csv',\n",
       " 'BAJFINANCE_with_indicators_.csv',\n",
       " 'BHARTIARTL_with_indicators_.csv',\n",
       " 'BPCL_with_indicators_.csv',\n",
       " 'BRITANNIA_with_indicators_.csv',\n",
       " 'CIPLA_with_indicators_.csv',\n",
       " 'COALINDIA_with_indicators_.csv',\n",
       " 'DIVISLAB_with_indicators_.csv',\n",
       " 'DRREDDY_with_indicators_.csv',\n",
       " 'EICHERMOT_with_indicators_.csv',\n",
       " 'GRASIM_with_indicators_.csv',\n",
       " 'HCLTECH_with_indicators_.csv',\n",
       " 'HDFCBANK_with_indicators_.csv',\n",
       " 'HDFCLIFE_with_indicators_.csv',\n",
       " 'HDFC_with_indicators_.csv',\n",
       " 'HEROMOTOCO_with_indicators_.csv',\n",
       " 'HINDALCO_with_indicators_.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('F:\\\\IITJ\\\\Semester_II\\\\SDE\\\\SDE Assignment 3\\\\Dataset\\\\Stock_market_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f58b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3253dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['ADANIPORTS_with_indicators_.csv',\n",
    "             'ASIANPAINT_with_indicators_.csv',\n",
    "             'AXISBANK_with_indicators_.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145aefa",
   "metadata": {},
   "source": [
    "file_list = ['ADANIPORTS_with_indicators_.csv',\n",
    " 'ASIANPAINT_with_indicators_.csv',\n",
    " 'AXISBANK_with_indicators_.csv',\n",
    " 'BAJAJFINSV_with_indicators_.csv',\n",
    " 'BAJFINANCE_with_indicators_.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80a026",
   "metadata": {},
   "source": [
    "file_list=['ADANIPORTS_with_indicators_.csv',\n",
    " 'ASIANPAINT_with_indicators_.csv',\n",
    " 'AXISBANK_with_indicators_.csv',\n",
    " 'BAJAJFINSV_with_indicators_.csv',\n",
    " 'BAJFINANCE_with_indicators_.csv',\n",
    " 'BHARTIARTL_with_indicators_.csv',\n",
    " 'BPCL_with_indicators_.csv',\n",
    " 'BRITANNIA_with_indicators_.csv',\n",
    " 'CIPLA_with_indicators_.csv',\n",
    " 'COALINDIA_with_indicators_.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84286e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADANIPORTS_with_indicators_.csv',\n",
       " 'ASIANPAINT_with_indicators_.csv',\n",
       " 'AXISBANK_with_indicators_.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f697f888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\IITJ\\\\Semester_II\\\\SDE\\\\SDE Assignment 3\\\\SDE_assignment_3_Final'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7543d56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'ADANIPORTS_with_indicators_.csv',\n",
       " 'ASIANPAINT_with_indicators_.csv',\n",
       " 'AXISBANK_with_indicators_.csv',\n",
       " 'BAJAJFINSV_with_indicators_.csv',\n",
       " 'BAJFINANCE_with_indicators_.csv',\n",
       " 'BHARTIARTL_with_indicators_.csv',\n",
       " 'BPCL_with_indicators_.csv',\n",
       " 'BRITANNIA_with_indicators_.csv',\n",
       " 'CIPLA_with_indicators_.csv',\n",
       " 'COALINDIA_with_indicators_.csv',\n",
       " 'pandas_timing_log.txt',\n",
       " 'pandas_timing_log_10_files.txt',\n",
       " 'pandas_timing_log_3_files.txt',\n",
       " 'pandas_timing_log_5_files.txt',\n",
       " 'SDE Assignment 3.pdf',\n",
       " 'SDE Assignment 3_Report_Subhajit_Saha_M21AIE261.docx',\n",
       " 'spark_timing.txt',\n",
       " 'spark_timing_log.txt',\n",
       " 'spark_timing_log_10_files.txt',\n",
       " 'spark_timing_log_3_files.txt',\n",
       " 'spark_timing_log_5_files.txt',\n",
       " 'Subhajit_Saha_SDE_Assignment_3_M21AIE261.ipynb',\n",
       " 'Subhajit_Saha_SDE_Assignment_3_M21AIE261.py',\n",
       " '~$E Assignment 3_Report_Subhajit_Saha_M21AIE261.docx']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec167a73",
   "metadata": {},
   "source": [
    "**BENCHMARKING QUERIES: \\\n",
    "Query 1: SELECT SUM(open) from data; \\\n",
    "Query 2: SELECT MAX(high) from data; \\\n",
    "Query 3: SELECT SUM(volume) from data;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae0e4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benchmark queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3760dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit function\n",
    "def spark_unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n",
    "\n",
    "def spark_query1(dfs):\n",
    "    return int(dfs.agg({\"open\": \"sum\"}).collect()[0][0])\n",
    "    \n",
    "def spark_query2(dfs):\n",
    "    return (dfs.agg({\"high\": \"max\"}).collect()[0][0])\n",
    "    \n",
    "def spark_query3(dfs):\n",
    "    return int(dfs.agg({\"volume\": \"sum\"}).collect()[0][0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09eb942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_benchmark(file_list):\n",
    "    spark_st = time.time()\n",
    "    spark = SparkSession.builder.appName(\n",
    "        'Read All CSV Files in Directory').getOrCreate()\n",
    "    spark_dfs = []\n",
    "    spark_file_size = 0\n",
    "    print(spark_file_size)\n",
    "    with open(\"spark_timing_log_3_files.txt\", \"w\") as fp:\n",
    "        for ind, s_file in enumerate(file_list):\n",
    "            print(s_file)\n",
    "            df = spark.read.option('delimiter', ',').csv(s_file, header=True)\n",
    "\n",
    "            file_size = os.path.getsize(s_file)\n",
    "            print(file_size)\n",
    "            print(file_list)\n",
    "            spark_file_size += file_size\n",
    "            spark_dfs.append(df)\n",
    "            fp.write(\"X----X-----X-----X----X\")\n",
    "            fp.write(\"\\n\")\n",
    "            fp.write(\"file name is {} and total file size becomes {} MB\".format(str(s_file), \n",
    "                                                                                str(spark_file_size / (1024*1024))))\n",
    "            \n",
    "            #print(str(round((spark_file_size / (1024*1024)),2)))\n",
    "            fp.write(\"\\n\")\n",
    "            temp_df = spark_unionAll(spark_dfs)\n",
    "            temp_et = time.time()\n",
    "\n",
    "            # get the execution time\n",
    "            t_elapsed_time = temp_et - spark_st\n",
    "            #fp.write(\"\\n\")\n",
    "            fp.write(\"Time takes to merge : {} seconds.\".format(str(t_elapsed_time)))\n",
    "            fp.write(\"\\n\")\n",
    "            q1_st = time.time()\n",
    "            \n",
    "            #fp.write(\"X----X-----X-----X----X\")\n",
    "            total_open = spark_query1(temp_df)\n",
    "            q1_et = time.time()\n",
    "            # get the execution time\n",
    "            q1_elapsed_time = q1_et - q1_st\n",
    "            total_q1_time = q1_et - spark_st\n",
    "            fp.write(\"To Run Query sum of opening price of stocks {} INR it takes {} seconds and Total Time Taken {} seconds.\".format(str(total_open),\n",
    "                                                                                               str(\n",
    "                                                                                                   q1_elapsed_time),\n",
    "                                                                                               str(total_q1_time)))\n",
    "            fp.write(\"\\n\")\n",
    "            #fp.write(\"X----X-----X-----X----X\")\n",
    "            q2_st = time.time()\n",
    "            max_high_price = spark_query2(temp_df)\n",
    "            q2_et = time.time()\n",
    "            q2_elapsed_time = q2_et - q2_st\n",
    "            total_q2_time = q2_et - spark_st\n",
    "            fp.write(\"To Run Query to get maximun high price of stocks {} INR it takes {} seconds and Total Time Taken {} seconds.\".format(str(max_high_price),\n",
    "                                                                                                 str(\n",
    "                                                                                                     q2_elapsed_time),\n",
    "                                                                                                 str(total_q2_time)))\n",
    "            fp.write(\"\\n\")\n",
    "            #fp.write(\"X----X-----X-----X----X\")\n",
    "            q3_st = time.time()\n",
    "            total_volume = spark_query3(temp_df)\n",
    "            q3_et = time.time()\n",
    "            q3_elapsed_time = q3_et - q3_st\n",
    "            total_q3_time = q3_et - spark_st\n",
    "            fp.write(\"To Run Query total volume of stocks {} INR it takes {} seconds and Total Time Taken {} seconds.\".format(str(total_volume),\n",
    "                                                                                            str(\n",
    "                                                                                                q3_elapsed_time),\n",
    "                                                                                            str(total_q3_time)))\n",
    "            fp.write(\"\\n\")\n",
    "            #fp.write(\"X----X-----X-----X----X\")\n",
    "        # spark_union_df = spark_unionAll(spark_dfs)\n",
    "\n",
    "        # print(spark_union_df.show())\n",
    "        spark_et = time.time()\n",
    "\n",
    "        # get the execution time\n",
    "        elapsed_time = spark_et - spark_st\n",
    "        elapsed_time=round(elapsed_time,2)\n",
    "        fp.write('Spark Execution time in total is: {} seconds'.format(str(elapsed_time)))\n",
    "        print(elapsed_time)\n",
    "        fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b610ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ADANIPORTS_with_indicators_.csv\n",
      "664071459\n",
      "['ADANIPORTS_with_indicators_.csv', 'ASIANPAINT_with_indicators_.csv', 'AXISBANK_with_indicators_.csv']\n",
      "ASIANPAINT_with_indicators_.csv\n",
      "664520087\n",
      "['ADANIPORTS_with_indicators_.csv', 'ASIANPAINT_with_indicators_.csv', 'AXISBANK_with_indicators_.csv']\n",
      "AXISBANK_with_indicators_.csv\n",
      "660470034\n",
      "['ADANIPORTS_with_indicators_.csv', 'ASIANPAINT_with_indicators_.csv', 'AXISBANK_with_indicators_.csv']\n",
      "418.09\n"
     ]
    }
   ],
   "source": [
    "## execute spark benchmark function\n",
    "spark_benchmark(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fb59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779db1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501b6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e8e1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benchmark queries for pandas\n",
    "\n",
    "def pd_query1(dfs):\n",
    "    return dfs.open.sum()\n",
    "\n",
    "def pd_query2(dfs):\n",
    "    return dfs.high.max()\n",
    "\n",
    "def pd_query3(dfs):\n",
    "    return dfs.volume.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16121d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_benchmark(file_list):\n",
    "    pd_st = time.time()\n",
    "    pd_dfs = []\n",
    "    pd_file_size = 0\n",
    "    print(pd_file_size)\n",
    "    with open(\"pandas_timing_log_3_files.txt\", \"w\") as fd:\n",
    "        for s_file in file_list:\n",
    "            print(s_file)\n",
    "            df = pd.read_csv(s_file)\n",
    "            pd_dfs.append(df)\n",
    "            file_size = os.path.getsize(s_file)\n",
    "            print(file_size)\n",
    "            pd_file_size += file_size\n",
    "            print(pd_file_size)\n",
    "            fd.write(\"X----X-----X-----X----X\")\n",
    "            fd.write(\"\\n\")\n",
    "            fd.write(\"file name is {} and total file size becomes {} MB\".format(str(s_file), \n",
    "                                                                                str(pd_file_size /(1024*1024))))\n",
    "            fd.write(\"\\n\")\n",
    "            temp_df = pd.concat(pd_dfs)\n",
    "            fd.write(\"total rows in dataframe is : {}\".format(str(len(temp_df))))\n",
    "            fd.write(\"\\n\")\n",
    "            # print(pd_union_df.head())\n",
    "\n",
    "            temp_et = time.time()\n",
    "\n",
    "            # get the execution time\n",
    "            #fd.write(\"X----X-----X-----X----X\")\n",
    "            t_elapsed_time = temp_et - pd_st\n",
    "            fd.write(\"Time takes to merge is {} seconds\".format(str(t_elapsed_time)))\n",
    "            fd.write(\"\\n\")\n",
    "            q1_st = time.time()\n",
    "            total_open = pd_query1(temp_df)\n",
    "            q1_et = time.time()\n",
    "            # get the execution time\n",
    "            q1_elapsed_time = q1_et - q1_st\n",
    "            total_q1_time = q1_et - pd_st\n",
    "            fd.write(\"To Run Query to get total sum of opening price of stocks : {} INR,  it takes {} seconds and Total Time Taken is {} seconds\".format(str(total_open),\n",
    "                                                                                               str(\n",
    "                                                                                                   q1_elapsed_time),\n",
    "                                                                                               str(total_q1_time)))\n",
    "            fd.write(\"\\n\")\n",
    "            #fd.write(\"X----X-----X-----X----X\")\n",
    "            q2_st = time.time()\n",
    "            max_high = pd_query2(temp_df)\n",
    "            q2_et = time.time()\n",
    "            q2_elapsed_time = q2_et - q2_st\n",
    "            total_q2_time = q2_et - pd_st\n",
    "            fd.write(\"To Run Query to get maximun high price of stocks : {} INR, it takes {} seconds and Total Time Taken is {} seconds\".format(str(max_high),\n",
    "                                                                                                 str(\n",
    "                                                                                                     q2_elapsed_time),\n",
    "                                                                                                 str(total_q2_time)))\n",
    "            fd.write(\"\\n\")\n",
    "            #fd.write(\"X----X-----X-----X----X\")\n",
    "            q3_st = time.time()\n",
    "            total_volume = pd_query3(temp_df)\n",
    "            q3_et = time.time()\n",
    "            q3_elapsed_time = q3_et - q3_st\n",
    "            total_q3_time = q3_et - pd_st\n",
    "            fd.write(\"To Run Query to get total volume of stocks: {}, it takes {} seconds and Total Time Taken is {} seconds\".format(str(total_volume),\n",
    "                                                                                            str(\n",
    "                                                                                                q3_elapsed_time),\n",
    "                                                                                            str(total_q3_time)))\n",
    "            fd.write(\"\\n\")\n",
    "            #fd.write(\"X----X-----X-----X----X\")\n",
    "        pd_et = time.time()\n",
    "\n",
    "        # get the execution time\n",
    "        elapsed_time = pd_et - pd_st\n",
    "        elapsed_time=round(elapsed_time,2)\n",
    "        fd.write('Pandas Execution time in total is: {} seconds'.format(str(elapsed_time)))\n",
    "        print(elapsed_time)\n",
    "        fd.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d24923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ADANIPORTS_with_indicators_.csv\n",
      "664071459\n",
      "664071459\n",
      "ASIANPAINT_with_indicators_.csv\n",
      "664520087\n",
      "1328591546\n",
      "AXISBANK_with_indicators_.csv\n",
      "660470034\n",
      "1989061580\n",
      "40.46\n"
     ]
    }
   ],
   "source": [
    "## execute pandas benchmark queries\n",
    "pandas_benchmark(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d2fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
